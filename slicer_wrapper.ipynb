{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adpanferov/anaconda3/envs/dist/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch, torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicingConfig():\n",
    "    def __init__(self, tensor_rules: dict, module_rules: dict):\n",
    "        self.tensor_rules = tensor_rules\n",
    "        self.module_rules = module_rules\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def slice_weight_vertical(tensor: torch.Tensor, rank: int, world_size: int) -> torch.Tensor:\n",
    "    assert(tensor.shape[-2] % world_size == 0)\n",
    "    slice_size = tensor.shape[-2] // world_size\n",
    "\n",
    "    return tensor[..., rank * slice_size: (rank + 1) * slice_size, :]\n",
    "\n",
    "def slice_bias_vertical(tensor: torch.Tensor, rank: int, world_size: int) -> torch.Tensor:\n",
    "    assert(tensor.shape[-1] % world_size == 0)\n",
    "    slice_size = tensor.shape[-1] // world_size\n",
    "\n",
    "    return tensor[rank * slice_size: (rank + 1) * slice_size]\n",
    "\n",
    "def slice_weight_horizontal(tensor: torch.Tensor, rank: int, world_size: int) -> torch.Tensor:\n",
    "    assert(tensor.shape[-1] % world_size == 0)\n",
    "    slice_size = tensor.shape[-1] // world_size\n",
    "\n",
    "    return tensor[..., rank * slice_size: (rank + 1) * slice_size]\n",
    "\n",
    "def slice_bias_horizontal(tensor: torch.Tensor, rank: int, world_size: int) -> torch.Tensor:\n",
    "    return tensor / world_size\n",
    "\n",
    "def slice_tensors(key_parameter_iterator, tensor_rules: dict, rank: int, world_size: int):\n",
    "    print(\"SLICING TENSORS\")\n",
    "    regular_rules = [(re.compile(\".*\" + key.replace(\".\", \"\\\\.\") + \"\\\\.(weight|bias)\"), value) for key, value in tensor_rules.items()]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, param in key_parameter_iterator:\n",
    "            for pattern, rule in regular_rules:\n",
    "                if not pattern.search(name) is None:\n",
    "                    name_ending = name.split('.')[-1]\n",
    "                    match (rule, name_ending):\n",
    "                        case (\"vertical\", \"weight\"):\n",
    "                            param.data = slice_weight_vertical(param.data, rank=rank, world_size=world_size)\n",
    "                        case (\"vertical\", \"bias\"):\n",
    "                            param.data = slice_bias_vertical(param.data, rank=rank, world_size=world_size)\n",
    "                        case (\"horizontal\", \"weight\"):\n",
    "                            param.data = slice_weight_horizontal(param.data, rank=rank, world_size=world_size)\n",
    "                        case (\"horizontal\", \"bias\"):\n",
    "                            param.data = slice_bias_horizontal(param.data, rank=rank, world_size=world_size)\n",
    "                        case _:\n",
    "                            raise Exception(\"Fuck you tensor!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bloom.modeling_bloom import BloomModel, BloomAttention, BloomMLP\n",
    "NAME = \"bigscience/bloom-560m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing_config = SlicingConfig(\n",
    "    {\n",
    "        \"self_attention.query_key_value\": \"vertical\",\n",
    "        \"self_attention.dense\": \"horizontal\",\n",
    "        \"mlp.dense_h_to_4h\": \"vertical\",\n",
    "        \"mlp.dense_4h_to_h\": \"horizontal\",\n",
    "    },\n",
    "    {\n",
    "        \"BloomAttention\": {\"input\": {\"alibi\": \"cut 0\", 1: \"scale\"}, \"output\": {\"ALL\": \"reduce\"}, \"attributes\": {\"num_heads\": \"scale_int\"}},\n",
    "        \"BloomMLP\": {\"input\": {1: \"scale\"}, \"output\": {0: \"reduce\"}, \"attributes\":{}},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.h[0].self_attention.query_key_value.weight.shape=torch.Size([3072, 1024])\n",
      "model.h[0].mlp.dense_h_to_4h.weight.shape=torch.Size([4096, 1024])\n",
      "SLICING TENSORS\n",
      "model.h[0].self_attention.query_key_value.weight.shape=torch.Size([1536, 1024])\n",
      "model.h[0].mlp.dense_h_to_4h.weight.shape=torch.Size([2048, 1024])\n"
     ]
    }
   ],
   "source": [
    "model = BloomModel.from_pretrained(NAME)\n",
    "print(f\"{model.h[0].self_attention.query_key_value.weight.shape=}\")\n",
    "print(f\"{model.h[0].mlp.dense_h_to_4h.weight.shape=}\")\n",
    "slice_tensors(model.named_parameters(), slicing_config.tensor_rules, 0, 2)\n",
    "print(f\"{model.h[0].self_attention.query_key_value.weight.shape=}\")\n",
    "print(f\"{model.h[0].mlp.dense_h_to_4h.weight.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "\n",
    "def process_input(rules, rank, world_size, *args, **kwargs):\n",
    "    args = list(args)\n",
    "    print(\"PROCESSING INPUT\")\n",
    "    for target, action in rules.items():\n",
    "        match action:\n",
    "            case \"cut\":\n",
    "                match target:\n",
    "                    case int(idx):\n",
    "                        slice_size= args[idx].shape[0] // world_size\n",
    "                        args[idx] = args[idx][..., rank * slice_size: (rank + 1) * slice_size, :]\n",
    "                    case str(name):\n",
    "                        slice_size= kwargs[name].shape[0] // world_size\n",
    "                        kwargs[name] = kwargs[name][rank * slice_size: (rank + 1) * slice_size, ...]\n",
    "                    case _:\n",
    "                        raise Exception(\"Fuck you cut input!\")\n",
    "\n",
    "            case \"scale\":\n",
    "                match target:\n",
    "                    case int(idx):\n",
    "                        args[idx] = args[idx] / world_size\n",
    "                    case str(name):\n",
    "                        kwargs[name] = kwargs[name]/ world_size\n",
    "                    case _:\n",
    "                        raise Exception(\"Fuck you scale input!\")\n",
    "            case _:\n",
    "                raise Exception(\"Fuck you input action!\")\n",
    "\n",
    "    print(kwargs[\"alibi\"].shape)\n",
    "    return args, kwargs\n",
    "\n",
    "def process_output(output, rules):\n",
    "    print(\"PROCESSING OUTPUT\")\n",
    "    for target, action in rules.items():\n",
    "        match action:\n",
    "            case \"reduce\":\n",
    "                match target:\n",
    "                    case \"ALL\":\n",
    "                        dist.all_reduce(output)\n",
    "                    case int(idx):\n",
    "                        dist.all_reduce(output[idx])\n",
    "                    case _:\n",
    "                        raise Exception(\"Fuck you output taget!\")\n",
    "            case _:\n",
    "                raise Exception(\"Fuck you output action!\")\n",
    "    return output\n",
    "\n",
    "def process_attr(module, rules, rank, world_size):\n",
    "    for attr, action in rules.items():\n",
    "            match action:\n",
    "                case \"scale_int\":\n",
    "                    setattr(module, attr, getattr(module, attr) // world_size)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelLayerWrapper(nn.Module):\n",
    "    def __init__(self, module: nn.Module, module_rules: dict, rank: int, world_size: int):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        process_attr(self.module, module_rules[\"attributes\"], rank=rank, world_size=world_size)\n",
    "\n",
    "        self.input_rules = module_rules[\"input\"]\n",
    "        self.output_rules = module_rules[\"output\"]\n",
    "\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        args, kwargs = process_input(self.input_rules, self.rank, self.world_size, *args, **kwargs)\n",
    "        output = self.module(*args, **kwargs)\n",
    "        return process_output(output, self.output_rules)\n",
    "\n",
    "\n",
    "def wrap_submodules(model: nn.Module, module_rules: dict, rank: int, world_size: int):\n",
    "    print(\"WRAPPING MODULES\")\n",
    "\n",
    "    unique_wrappers = {module: ParallelLayerWrapper(module, module_rules[type(module).__name__], rank=rank, world_size=world_size) for module in model.modules() if type(module).__name__ in module_rules}\n",
    "\n",
    "\n",
    "    for parent in list(model.modules()):\n",
    "        for child_name, child in list(parent.named_children()):\n",
    "            if child in unique_wrappers:\n",
    "                setattr(parent, child_name, unique_wrappers[child])\n",
    "\n",
    "    # for module in model.modules():\n",
    "    #     if type(module).__name__ in module_rules:\n",
    "    #         module = ParallelLayerWrapper(module, module_rules[type(module).__name__], rank=rank, world_size=world_size)\n",
    "\n",
    "def tensor_parallel(model_cls, slicing_config: SlicingConfig, rank: int, world_size: int):\n",
    "    global SLICING_CONFIG\n",
    "    SLICING_CONFIG = slicing_config\n",
    "\n",
    "    global RANK\n",
    "    RANK = rank\n",
    "\n",
    "    global WORLD_SIZE\n",
    "    WORLD_SIZE = world_size\n",
    "\n",
    "    class _TensorParallel(model_cls):\n",
    "        def __new__(cls, *args, **kwargs):\n",
    "            model = model_cls(*args, **kwargs)  # Create an instance of vanilla model\n",
    "            \n",
    "            # modify untrained parameters/buffers\n",
    "            slice_tensors(model.named_parameters(), SLICING_CONFIG.tensor_rules, RANK, WORLD_SIZE)\n",
    "\n",
    "            return model\n",
    "\n",
    "        @classmethod\n",
    "        def _load_pretrained_model(cls, model: model_cls, state_dict, loaded_keys, *args, **kwargs):\n",
    "            slice_tensors(state_dict.items(), SLICING_CONFIG.tensor_rules, RANK, WORLD_SIZE)\n",
    "            result = super()._load_pretrained_model(model, state_dict, loaded_keys, *args, **kwargs)\n",
    "            \n",
    "            wrap_submodules(model, SLICING_CONFIG.module_rules, RANK, WORLD_SIZE)\n",
    "\n",
    "            return result\n",
    "        \n",
    "    return _TensorParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLICING TENSORS\n",
      "SLICING TENSORS\n",
      "WRAPPING MODULES\n"
     ]
    }
   ],
   "source": [
    "m = tensor_parallel(BloomModel, slicing_config, 0, 2).from_pretrained(NAME)\n",
    "assert isinstance(m, BloomModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING INPUT\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Fuck you input action!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbeleriand/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m test_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m]])\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbeleriand/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m m(test_input)\n",
      "File \u001b[0;32m~/anaconda3/envs/dist/lib/python3.10/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1424\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dist/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:747\u001b[0m, in \u001b[0;36mBloomModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    739\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    740\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    741\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    744\u001b[0m         head_mask[i],\n\u001b[1;32m    745\u001b[0m     )\n\u001b[1;32m    746\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 747\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    748\u001b[0m         hidden_states,\n\u001b[1;32m    749\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    750\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m    751\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    752\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    753\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    754\u001b[0m         alibi\u001b[39m=\u001b[39;49malibi,\n\u001b[1;32m    755\u001b[0m     )\n\u001b[1;32m    757\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dist/lib/python3.10/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1424\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dist/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:442\u001b[0m, in \u001b[0;36mBloomBlock.forward\u001b[0;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    439\u001b[0m     residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    441\u001b[0m \u001b[39m# Self attention.\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention(\n\u001b[1;32m    443\u001b[0m     layernorm_output,\n\u001b[1;32m    444\u001b[0m     residual,\n\u001b[1;32m    445\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    446\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    447\u001b[0m     alibi\u001b[39m=\u001b[39;49malibi,\n\u001b[1;32m    448\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    449\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    450\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    451\u001b[0m )\n\u001b[1;32m    453\u001b[0m attention_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    455\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/dist/lib/python3.10/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1424\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb Cell 10\u001b[0m in \u001b[0;36mParallelLayerWrapper.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeleriand/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbeleriand/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     args, kwargs \u001b[39m=\u001b[39m process_input(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_rules, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrank, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworld_size, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeleriand/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeleriand/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m process_output(output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_rules)\n",
      "\u001b[1;32m/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb Cell 10\u001b[0m in \u001b[0;36mprocess_input\u001b[0;34m(rules, rank, world_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeleriand/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m                     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFuck you scale input!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeleriand/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39mcase\u001b[39;00m \u001b[39m_\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbeleriand/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFuck you input action!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeleriand/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39malibi\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeleriand/home/adpanferov/petals_local_parallel/slicer_wrapper.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mreturn\u001b[39;00m args, kwargs\n",
      "\u001b[0;31mException\u001b[0m: Fuck you input action!"
     ]
    }
   ],
   "source": [
    "test_input = torch.tensor([[1, 2, 3, 4, 5]])\n",
    "m(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dist')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4f9af776eba44096d2a2b91b2ae4dd4ecd10ec5726c41598312832383f4ab0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
